{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.array((x - np.min(x))/(np.max(x) - np.min(x)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32,[None,*image_shape],name = \"x\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    y = tf.placeholder(tf.float32,[None,n_classes],name = \"y\")\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32,name = \"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    filter_height = conv_ksize[0]\n",
    "    filter_width = conv_ksize[1]\n",
    "    color_channels = x_tensor.get_shape().as_list()[3]\n",
    "    # Weights & bias\n",
    "    weights = tf.Variable(tf.truncated_normal([filter_height, filter_width, color_channels, conv_num_outputs],\n",
    "                                              mean=0.0, stddev=0.1, dtype = tf.float32))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs, dtype = tf.float32))\n",
    "    # Apply Convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1,*conv_strides,1], padding = 'SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    # Apply Max Pooling\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, *pool_ksize, 1],    \n",
    "    strides=[1, *pool_strides, 1],\n",
    "    #ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "    #strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "    padding = 'SAME')\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size = x_tensor.get_shape().as_list()[0] \n",
    "    width = x_tensor.get_shape().as_list()[1]\n",
    "    height = x_tensor.get_shape().as_list()[2]\n",
    "    depth = x_tensor.get_shape().as_list()[3]\n",
    "    \n",
    "    flat_image_size = width*height*depth\n",
    "    return tf.contrib.layers.flatten(x_tensor, [batch_size,flat_image_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_size = x_tensor.get_shape().as_list()[0]\n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs],\n",
    "    #                                         mean=0.0, stddev = 0.1, dtype = tf.float32)) \n",
    "    #linear_out = tf.matmul(x_tensor,weights,name='linear_out')\n",
    "    #bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    #output = tf.add(linear_out,bias)\n",
    "    output_layer = tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "    return output_layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = [64,128]\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    \n",
    "    #2 convolution layers\n",
    "    conv_layerA = conv2d_maxpool(x_tensor, conv_num_outputs[0], conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layerB = conv2d_maxpool(conv_layerA, conv_num_outputs[1], conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "   \n",
    "    flatten_layer = flatten(conv_layerB)\n",
    "\n",
    "    fully_connected_layerA = fully_conn(flatten_layer, 64)\n",
    "    fully_connected_layerA = tf.nn.dropout(fully_connected_layerA, keep_prob)\n",
    "    #fully_connected_layerB = fully_conn(fully_connected_layerA, 128)\n",
    "    #fully_connected_layerB = tf.nn.dropout(fully_connected_layerB, keep_prob)\n",
    "\n",
    "    return output(fully_connected_layerA, 10)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict = {x: feature_batch, y: label_batch, keep_prob:1.0} )\n",
    "    \n",
    "    valid_acc = session.run(accuracy, feed_dict = {x: valid_features, y: valid_labels, keep_prob:1.0})\n",
    "    \n",
    "    print('Loss: {:>10.4f} Accuracy: {:.6f}'.format(loss,valid_acc))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.1517 Accuracy: 0.251400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0086 Accuracy: 0.310400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.9338 Accuracy: 0.344600\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.7413 Accuracy: 0.401600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.5949 Accuracy: 0.428000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.5524 Accuracy: 0.406400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.4006 Accuracy: 0.445800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.3038 Accuracy: 0.469600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.2302 Accuracy: 0.474400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.1609 Accuracy: 0.500800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.1297 Accuracy: 0.484600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.0536 Accuracy: 0.497800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.0209 Accuracy: 0.513600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.9555 Accuracy: 0.528600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.9046 Accuracy: 0.530600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.8480 Accuracy: 0.544200\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.8306 Accuracy: 0.535800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.7447 Accuracy: 0.549600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.7799 Accuracy: 0.536400\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.7032 Accuracy: 0.522400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.6308 Accuracy: 0.559600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.5738 Accuracy: 0.544800\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.6024 Accuracy: 0.507000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.5828 Accuracy: 0.556600\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.4912 Accuracy: 0.551600\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.4590 Accuracy: 0.573200\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.4064 Accuracy: 0.556200\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.3757 Accuracy: 0.568200\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.3936 Accuracy: 0.572600\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.3707 Accuracy: 0.558800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.3097 Accuracy: 0.576000\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.2736 Accuracy: 0.579400\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.2794 Accuracy: 0.576800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.2658 Accuracy: 0.543600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.2635 Accuracy: 0.535400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.2645 Accuracy: 0.558600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.2507 Accuracy: 0.548200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.2405 Accuracy: 0.511000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.2262 Accuracy: 0.568200\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.2155 Accuracy: 0.563000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.1702 Accuracy: 0.556000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.1825 Accuracy: 0.584200\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.1609 Accuracy: 0.576400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.1298 Accuracy: 0.589600\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.1632 Accuracy: 0.576800\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.1430 Accuracy: 0.569600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.1463 Accuracy: 0.565600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.1174 Accuracy: 0.562400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.1048 Accuracy: 0.577600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.1266 Accuracy: 0.557600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.1068 Accuracy: 0.567200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.1018 Accuracy: 0.572200\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.0769 Accuracy: 0.574600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.0747 Accuracy: 0.575200\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.0724 Accuracy: 0.575800\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.0654 Accuracy: 0.574000\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.0911 Accuracy: 0.563600\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.0633 Accuracy: 0.579600\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.0672 Accuracy: 0.574200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.0611 Accuracy: 0.573600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.0525 Accuracy: 0.569000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.0451 Accuracy: 0.575200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.0597 Accuracy: 0.563400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.0464 Accuracy: 0.551400\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0556 Accuracy: 0.545800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0541 Accuracy: 0.525600\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.0411 Accuracy: 0.566200\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0414 Accuracy: 0.553200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0376 Accuracy: 0.555800\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0486 Accuracy: 0.542200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0292 Accuracy: 0.562600\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0490 Accuracy: 0.533400\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0202 Accuracy: 0.563600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0349 Accuracy: 0.569800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0307 Accuracy: 0.566200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0246 Accuracy: 0.558200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0275 Accuracy: 0.563600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0208 Accuracy: 0.574400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0202 Accuracy: 0.561400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0202 Accuracy: 0.562200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0160 Accuracy: 0.558200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0358 Accuracy: 0.549000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0193 Accuracy: 0.561400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0178 Accuracy: 0.534000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0235 Accuracy: 0.548600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0118 Accuracy: 0.556200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0152 Accuracy: 0.553600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0313 Accuracy: 0.541600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0201 Accuracy: 0.556400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0178 Accuracy: 0.532800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0250 Accuracy: 0.545600\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0134 Accuracy: 0.556200\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0208 Accuracy: 0.547200\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0143 Accuracy: 0.545400\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0153 Accuracy: 0.547600\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0143 Accuracy: 0.571600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0293 Accuracy: 0.567400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0117 Accuracy: 0.553400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0063 Accuracy: 0.575000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0043 Accuracy: 0.559800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2014 Accuracy: 0.215400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.0873 Accuracy: 0.274200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.8394 Accuracy: 0.322000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.8096 Accuracy: 0.333600\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.6853 Accuracy: 0.348600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.8044 Accuracy: 0.367600\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.8237 Accuracy: 0.392400\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.4493 Accuracy: 0.381600\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.6134 Accuracy: 0.411000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.4092 Accuracy: 0.437800\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.6383 Accuracy: 0.446800\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.5557 Accuracy: 0.483000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.1984 Accuracy: 0.493800\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.4303 Accuracy: 0.495000\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.2737 Accuracy: 0.505400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.4528 Accuracy: 0.505200\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.2311 Accuracy: 0.536600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.9681 Accuracy: 0.539800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.2379 Accuracy: 0.541400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.1520 Accuracy: 0.559400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.2122 Accuracy: 0.564000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.1019 Accuracy: 0.571800\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.8962 Accuracy: 0.567000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.0690 Accuracy: 0.568200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.0221 Accuracy: 0.577400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.1131 Accuracy: 0.589200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.0071 Accuracy: 0.576200\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.8145 Accuracy: 0.597400\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.9840 Accuracy: 0.581800\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.9095 Accuracy: 0.578200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.0050 Accuracy: 0.580600\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.9024 Accuracy: 0.593600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.7284 Accuracy: 0.612000\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.8746 Accuracy: 0.606000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.8687 Accuracy: 0.594200\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.8555 Accuracy: 0.596600\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.8276 Accuracy: 0.613600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.6465 Accuracy: 0.606800\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.7316 Accuracy: 0.611000\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.7052 Accuracy: 0.625600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.7476 Accuracy: 0.634400\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.7584 Accuracy: 0.625600\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.6060 Accuracy: 0.620600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.7075 Accuracy: 0.621800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.6323 Accuracy: 0.634600\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.7335 Accuracy: 0.638200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.6449 Accuracy: 0.620800\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.5428 Accuracy: 0.635400\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.6516 Accuracy: 0.633400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.5562 Accuracy: 0.641200\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.6590 Accuracy: 0.636800\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.5728 Accuracy: 0.631200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.4347 Accuracy: 0.631600\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.6076 Accuracy: 0.651400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.5300 Accuracy: 0.646400\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.5851 Accuracy: 0.656200\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.5552 Accuracy: 0.648000\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.4228 Accuracy: 0.639200\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.4907 Accuracy: 0.657400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.4639 Accuracy: 0.649200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.5308 Accuracy: 0.653000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.4660 Accuracy: 0.654800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.3459 Accuracy: 0.647600\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.4781 Accuracy: 0.663600\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.4469 Accuracy: 0.640600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.5447 Accuracy: 0.641800\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.4186 Accuracy: 0.647400\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.3690 Accuracy: 0.652000\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.4579 Accuracy: 0.654200\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.4003 Accuracy: 0.649400\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.4594 Accuracy: 0.646600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.4220 Accuracy: 0.658800\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.3173 Accuracy: 0.639600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.4828 Accuracy: 0.649800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.3897 Accuracy: 0.663800\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.3890 Accuracy: 0.659400\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.4213 Accuracy: 0.663400\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.3055 Accuracy: 0.643600\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.4200 Accuracy: 0.654200\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.3551 Accuracy: 0.665200\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.4009 Accuracy: 0.656600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.4713 Accuracy: 0.665000\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.2879 Accuracy: 0.650000\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.4118 Accuracy: 0.658200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.3252 Accuracy: 0.671200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.3172 Accuracy: 0.669000\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.3645 Accuracy: 0.663200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.2677 Accuracy: 0.655000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.3753 Accuracy: 0.654400\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.3014 Accuracy: 0.663200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.3152 Accuracy: 0.671200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.3287 Accuracy: 0.664400\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.2442 Accuracy: 0.649400\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.3531 Accuracy: 0.653200\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.2982 Accuracy: 0.654200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.2990 Accuracy: 0.672200\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.2952 Accuracy: 0.664200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.2093 Accuracy: 0.652800\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.3080 Accuracy: 0.671600\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.2526 Accuracy: 0.646600\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.2515 Accuracy: 0.676000\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.3033 Accuracy: 0.661400\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.2375 Accuracy: 0.673800\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.2878 Accuracy: 0.653600\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.2079 Accuracy: 0.663800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.2499 Accuracy: 0.675200\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.2621 Accuracy: 0.678400\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.2043 Accuracy: 0.662800\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.2430 Accuracy: 0.661400\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.1980 Accuracy: 0.669000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.2341 Accuracy: 0.679400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.3041 Accuracy: 0.670000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.1944 Accuracy: 0.672800\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.2485 Accuracy: 0.660000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.2052 Accuracy: 0.660400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.2112 Accuracy: 0.674600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.2759 Accuracy: 0.670000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.1827 Accuracy: 0.670800\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.2508 Accuracy: 0.645600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.1601 Accuracy: 0.658000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.2333 Accuracy: 0.662000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.2834 Accuracy: 0.669800\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.1862 Accuracy: 0.674400\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.2786 Accuracy: 0.638400\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.1510 Accuracy: 0.660800\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.1749 Accuracy: 0.672200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.2403 Accuracy: 0.663000\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.1578 Accuracy: 0.662800\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.2184 Accuracy: 0.651600\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.1373 Accuracy: 0.678800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.2094 Accuracy: 0.668400\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.2506 Accuracy: 0.678800\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.1532 Accuracy: 0.662000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.2156 Accuracy: 0.648600\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.1454 Accuracy: 0.676200\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.1858 Accuracy: 0.670400\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.2305 Accuracy: 0.667400\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.1331 Accuracy: 0.666000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.1948 Accuracy: 0.640800\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.1275 Accuracy: 0.678800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.1548 Accuracy: 0.655800\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.2113 Accuracy: 0.660600\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.1222 Accuracy: 0.653200\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.2061 Accuracy: 0.656200\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.1381 Accuracy: 0.672800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.1463 Accuracy: 0.669000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.2086 Accuracy: 0.656800\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.1249 Accuracy: 0.665400\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.1899 Accuracy: 0.643400\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.1145 Accuracy: 0.666200\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.1684 Accuracy: 0.672400\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.2108 Accuracy: 0.662200\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.1576 Accuracy: 0.655000\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.1820 Accuracy: 0.643400\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.0871 Accuracy: 0.682200\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.1659 Accuracy: 0.662600\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.2074 Accuracy: 0.662400\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.0962 Accuracy: 0.657600\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.1402 Accuracy: 0.668800\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.0889 Accuracy: 0.678000\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.1380 Accuracy: 0.664800\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.1849 Accuracy: 0.664400\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.1094 Accuracy: 0.658600\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.1491 Accuracy: 0.658600\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.1215 Accuracy: 0.671400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.1504 Accuracy: 0.656000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.1797 Accuracy: 0.663200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.1028 Accuracy: 0.664000\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.1472 Accuracy: 0.648400\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.0960 Accuracy: 0.671000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.1466 Accuracy: 0.650600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.1718 Accuracy: 0.664400\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.0782 Accuracy: 0.666600\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.1464 Accuracy: 0.652800\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.1199 Accuracy: 0.669400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.1364 Accuracy: 0.658000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.1804 Accuracy: 0.668400\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.0891 Accuracy: 0.671200\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.1088 Accuracy: 0.655600\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.0731 Accuracy: 0.670600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.1475 Accuracy: 0.645800\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.1764 Accuracy: 0.677000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.0844 Accuracy: 0.670000\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.0989 Accuracy: 0.660600\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.0893 Accuracy: 0.671800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.1298 Accuracy: 0.668200\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.1904 Accuracy: 0.670800\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.0773 Accuracy: 0.650600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.0794 Accuracy: 0.661200\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.0718 Accuracy: 0.665400\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.0939 Accuracy: 0.667000\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.1745 Accuracy: 0.665800\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.0915 Accuracy: 0.662600\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.1005 Accuracy: 0.646000\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.0824 Accuracy: 0.661400\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.1353 Accuracy: 0.660800\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.1974 Accuracy: 0.657400\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.0939 Accuracy: 0.650400\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.0830 Accuracy: 0.659600\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.0662 Accuracy: 0.663000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.1351 Accuracy: 0.666200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.1799 Accuracy: 0.662600\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.0579 Accuracy: 0.647200\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.0818 Accuracy: 0.654600\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.0720 Accuracy: 0.659400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.1145 Accuracy: 0.641600\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.1695 Accuracy: 0.668200\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.0633 Accuracy: 0.657000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.0811 Accuracy: 0.660800\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.0568 Accuracy: 0.664000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.1058 Accuracy: 0.647800\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.1322 Accuracy: 0.671200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.0805 Accuracy: 0.639600\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.0700 Accuracy: 0.651200\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.0691 Accuracy: 0.674400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.1084 Accuracy: 0.649600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.1350 Accuracy: 0.661600\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.0503 Accuracy: 0.648600\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.0883 Accuracy: 0.656000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.0384 Accuracy: 0.657400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.1264 Accuracy: 0.647200\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.1315 Accuracy: 0.662600\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.0543 Accuracy: 0.647400\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.0844 Accuracy: 0.645000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.0567 Accuracy: 0.650000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.1120 Accuracy: 0.662000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.1118 Accuracy: 0.674600\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.0336 Accuracy: 0.645400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.0691 Accuracy: 0.647000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.0571 Accuracy: 0.652600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.0646 Accuracy: 0.653400\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.0937 Accuracy: 0.665200\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.0427 Accuracy: 0.644000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.0507 Accuracy: 0.662200\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.0463 Accuracy: 0.658000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.0794 Accuracy: 0.657800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.1183 Accuracy: 0.663200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.0455 Accuracy: 0.662600\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.0513 Accuracy: 0.649000\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.0262 Accuracy: 0.662800\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.0860 Accuracy: 0.663800\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.1053 Accuracy: 0.663000\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.0477 Accuracy: 0.645600\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.0708 Accuracy: 0.649600\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.0542 Accuracy: 0.659400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.0905 Accuracy: 0.669200\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.0797 Accuracy: 0.657200\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.0399 Accuracy: 0.653800\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.0433 Accuracy: 0.657800\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.0398 Accuracy: 0.644600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.0746 Accuracy: 0.670800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.0893 Accuracy: 0.664800\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.0274 Accuracy: 0.663200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.0428 Accuracy: 0.663400\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.0250 Accuracy: 0.668400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.0618 Accuracy: 0.673600\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.0905 Accuracy: 0.659200\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.0278 Accuracy: 0.664800\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.0542 Accuracy: 0.643000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.0310 Accuracy: 0.658600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.0588 Accuracy: 0.671000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.0723 Accuracy: 0.663200\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.0478 Accuracy: 0.649600\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.0427 Accuracy: 0.664000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.0410 Accuracy: 0.666000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1034 Accuracy: 0.670000\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.0599 Accuracy: 0.655800\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.0303 Accuracy: 0.666400\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.0620 Accuracy: 0.651800\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.0459 Accuracy: 0.658000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.0718 Accuracy: 0.661600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.0529 Accuracy: 0.655400\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.0430 Accuracy: 0.665600\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.0369 Accuracy: 0.663200\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.0420 Accuracy: 0.638400\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.0632 Accuracy: 0.664200\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.0435 Accuracy: 0.651200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.0329 Accuracy: 0.661200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.0304 Accuracy: 0.652200\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.0375 Accuracy: 0.654200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.0652 Accuracy: 0.666400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.0378 Accuracy: 0.645400\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.0404 Accuracy: 0.655200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.0444 Accuracy: 0.651200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.0334 Accuracy: 0.659400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.1122 Accuracy: 0.652800\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.0417 Accuracy: 0.655400\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.0174 Accuracy: 0.659800\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.0444 Accuracy: 0.661800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.0393 Accuracy: 0.645200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.1157 Accuracy: 0.653800\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.0481 Accuracy: 0.654600\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.0244 Accuracy: 0.661200\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.0365 Accuracy: 0.660800\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.0227 Accuracy: 0.672200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.0704 Accuracy: 0.652600\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.0410 Accuracy: 0.644800\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.0205 Accuracy: 0.661600\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.0597 Accuracy: 0.661400\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.0163 Accuracy: 0.662600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.0631 Accuracy: 0.649400\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.0392 Accuracy: 0.636000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.0167 Accuracy: 0.655800\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.0320 Accuracy: 0.665600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.0187 Accuracy: 0.656200\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.0607 Accuracy: 0.650600\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.0299 Accuracy: 0.639400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.0177 Accuracy: 0.654400\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.0750 Accuracy: 0.653600\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.0158 Accuracy: 0.667400\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.0534 Accuracy: 0.658200\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.0264 Accuracy: 0.639800\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.0176 Accuracy: 0.649600\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.0253 Accuracy: 0.670200\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.0144 Accuracy: 0.664800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.0398 Accuracy: 0.651400\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.0274 Accuracy: 0.641400\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.0180 Accuracy: 0.660200\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.0231 Accuracy: 0.657400\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.0217 Accuracy: 0.663000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0840 Accuracy: 0.657000\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.0475 Accuracy: 0.651800\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.0083 Accuracy: 0.649400\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.0268 Accuracy: 0.660800\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.0324 Accuracy: 0.663800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0560 Accuracy: 0.650600\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.0321 Accuracy: 0.637600\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.0101 Accuracy: 0.651000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.0378 Accuracy: 0.650000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.0156 Accuracy: 0.663200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.0762 Accuracy: 0.652600\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.0404 Accuracy: 0.621200\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.0141 Accuracy: 0.652400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.0211 Accuracy: 0.657400\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.0264 Accuracy: 0.664200\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0422 Accuracy: 0.656800\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.0238 Accuracy: 0.626200\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.0147 Accuracy: 0.657800\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.0156 Accuracy: 0.647000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.0423 Accuracy: 0.648600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0500 Accuracy: 0.658000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.0236 Accuracy: 0.630800\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.0112 Accuracy: 0.649600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.0173 Accuracy: 0.659800\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.0147 Accuracy: 0.653400\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0642 Accuracy: 0.656600\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.0556 Accuracy: 0.616400\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.0097 Accuracy: 0.656200\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.0227 Accuracy: 0.661800\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.0144 Accuracy: 0.660800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0616 Accuracy: 0.656000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.0298 Accuracy: 0.638200\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.0103 Accuracy: 0.654000\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.0616 Accuracy: 0.642600\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.0111 Accuracy: 0.657800\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0881 Accuracy: 0.651000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.0214 Accuracy: 0.644600\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.0047 Accuracy: 0.654800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.0200 Accuracy: 0.646600\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.0140 Accuracy: 0.664400\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0547 Accuracy: 0.648600\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.0248 Accuracy: 0.636600\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.0062 Accuracy: 0.652600\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.0155 Accuracy: 0.643800\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.0300 Accuracy: 0.653600\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0645 Accuracy: 0.651600\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.0184 Accuracy: 0.622000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.0092 Accuracy: 0.651600\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.0480 Accuracy: 0.635600\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.0220 Accuracy: 0.662600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0490 Accuracy: 0.660800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.0209 Accuracy: 0.643800\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.0184 Accuracy: 0.641600\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.0105 Accuracy: 0.642200\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.0137 Accuracy: 0.658400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0514 Accuracy: 0.664600\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.0145 Accuracy: 0.633800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.0069 Accuracy: 0.649800\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.0263 Accuracy: 0.641800\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.0171 Accuracy: 0.663600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0461 Accuracy: 0.655200\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.0287 Accuracy: 0.621600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.0293 Accuracy: 0.644800\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.0201 Accuracy: 0.630600\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.0189 Accuracy: 0.662200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0535 Accuracy: 0.663200\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.0130 Accuracy: 0.642000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.0144 Accuracy: 0.652600\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.0222 Accuracy: 0.644000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.0247 Accuracy: 0.656600\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0623 Accuracy: 0.652600\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.0186 Accuracy: 0.638400\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.0435 Accuracy: 0.649800\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.0216 Accuracy: 0.633600\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.0184 Accuracy: 0.655000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0557 Accuracy: 0.655600\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.0179 Accuracy: 0.621400\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.0109 Accuracy: 0.648400\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.0186 Accuracy: 0.611200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.0211 Accuracy: 0.655200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0455 Accuracy: 0.654400\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.0158 Accuracy: 0.625400\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.0037 Accuracy: 0.656800\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.0095 Accuracy: 0.628000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.0100 Accuracy: 0.659400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0580 Accuracy: 0.657600\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.0191 Accuracy: 0.633400\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.0038 Accuracy: 0.659000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.0112 Accuracy: 0.633600\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.0117 Accuracy: 0.652400\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0548 Accuracy: 0.652600\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.0328 Accuracy: 0.636400\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.0085 Accuracy: 0.656400\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.0215 Accuracy: 0.639400\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.0097 Accuracy: 0.657400\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0523 Accuracy: 0.659800\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.0289 Accuracy: 0.630600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.0075 Accuracy: 0.661400\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.0133 Accuracy: 0.634200\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.0123 Accuracy: 0.659400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0564 Accuracy: 0.646400\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.0132 Accuracy: 0.643600\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.0048 Accuracy: 0.648000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.0078 Accuracy: 0.647000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.0165 Accuracy: 0.654800\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0486 Accuracy: 0.645200\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.0191 Accuracy: 0.640200\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.0049 Accuracy: 0.653000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.0151 Accuracy: 0.638000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.0097 Accuracy: 0.658600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0927 Accuracy: 0.636600\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.0104 Accuracy: 0.648000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.0051 Accuracy: 0.659000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.0149 Accuracy: 0.625800\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.0120 Accuracy: 0.654400\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0765 Accuracy: 0.646400\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.0142 Accuracy: 0.646200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.0077 Accuracy: 0.652200\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.0073 Accuracy: 0.642600\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.0202 Accuracy: 0.652200\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0600 Accuracy: 0.650200\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.0111 Accuracy: 0.644800\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.0051 Accuracy: 0.649000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.0123 Accuracy: 0.629600\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.0070 Accuracy: 0.651000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0572 Accuracy: 0.649200\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.0103 Accuracy: 0.642800\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.0026 Accuracy: 0.644800\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.0180 Accuracy: 0.639600\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.0122 Accuracy: 0.656600\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0802 Accuracy: 0.641800\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.0112 Accuracy: 0.650000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.0065 Accuracy: 0.659000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.0089 Accuracy: 0.641000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.0084 Accuracy: 0.651000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0625 Accuracy: 0.649800\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.0138 Accuracy: 0.648200\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.0060 Accuracy: 0.653600\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.0074 Accuracy: 0.645200\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.0097 Accuracy: 0.655400\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0629 Accuracy: 0.648000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.0185 Accuracy: 0.645200\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.0038 Accuracy: 0.646600\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.0435 Accuracy: 0.642200\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.0727 Accuracy: 0.645800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0592 Accuracy: 0.655600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.0215 Accuracy: 0.653400\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.0033 Accuracy: 0.646600\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.0197 Accuracy: 0.645000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.0084 Accuracy: 0.647800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0571 Accuracy: 0.656400\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.0147 Accuracy: 0.650800\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.0059 Accuracy: 0.660200\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.0075 Accuracy: 0.642000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.0168 Accuracy: 0.653400\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0671 Accuracy: 0.650800\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.0063 Accuracy: 0.636000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.0034 Accuracy: 0.646600\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.0124 Accuracy: 0.645200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.0198 Accuracy: 0.640000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0356 Accuracy: 0.651800\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.0092 Accuracy: 0.644400\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.0054 Accuracy: 0.660000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.0085 Accuracy: 0.650600\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.0135 Accuracy: 0.649400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0504 Accuracy: 0.652600\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.0056 Accuracy: 0.643200\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.0049 Accuracy: 0.649400\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.0143 Accuracy: 0.634200\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.0166 Accuracy: 0.651400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0200 Accuracy: 0.659800\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.0125 Accuracy: 0.653000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.0050 Accuracy: 0.650600\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.0101 Accuracy: 0.648800\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.0264 Accuracy: 0.643800\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0153 Accuracy: 0.657800\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.0179 Accuracy: 0.644400\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.0057 Accuracy: 0.644600\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.0210 Accuracy: 0.648200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.0098 Accuracy: 0.652200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6644580696202531\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe2eb5f0d30>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
